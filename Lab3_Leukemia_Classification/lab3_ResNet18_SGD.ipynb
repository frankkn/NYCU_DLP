{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a93e611",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import pickle\n",
    "\n",
    "def getData(mode):\n",
    "    if mode == 'train':\n",
    "        df = pd.read_csv('train.csv')\n",
    "        path = df['Path'].tolist()\n",
    "        label = df['label'].tolist()\n",
    "        return path, label\n",
    "    \n",
    "    elif mode == 'valid':\n",
    "        df = pd.read_csv('valid.csv')\n",
    "        path = df['Path'].tolist()\n",
    "        label = df['label'].tolist()\n",
    "        return path, label\n",
    "    \n",
    "    else:\n",
    "        df = pd.read_csv('resnet_18_test.csv')\n",
    "        path = df['Path'].tolist()\n",
    "        return path\n",
    "\n",
    "class RetinopathyLoader(data.Dataset):\n",
    "    def __init__(self, root, mode):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root (string): Root path of the dataset.\n",
    "            mode : Indicate procedure status(training or testing)\n",
    "\n",
    "            self.img_name (string list): String list that store all image names.\n",
    "            self.label (int or float list): Numerical list that store all ground truth label values.\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.mode = mode\n",
    "        \n",
    "        if mode == 'train' or mode == 'valid':\n",
    "            self.img_name, self.label = getData(mode)\n",
    "        else:  # mode == 'test'\n",
    "            self.img_name = getData(mode)\n",
    "            self.label = None  # No labels for test data\n",
    "            \n",
    "        # print(\"> Found %d images...\" % (len(self.img_name)))\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"'return the size of dataset\"\"\"\n",
    "        return len(self.img_name)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        \"\"\"\n",
    "           step1. Get the image path from 'self.img_name' and load it.\n",
    "                  hint : path = root + self.img_name[index] + '.jpeg'\n",
    "\n",
    "           step2. Get the ground truth label from self.label\n",
    "\n",
    "           step3. Transform the .jpeg rgb images during the training phase, such as resizing, random flipping, \n",
    "                  rotation, cropping, normalization etc. But at the beginning, I suggest you follow the hints. \n",
    "\n",
    "                  In the testing phase, if you have a normalization process during the training phase, you only need \n",
    "                  to normalize the data. \n",
    "\n",
    "                  hints : Convert the pixel value to [0, 1]\n",
    "                          Transpose the image shape from [H, W, C] to [C, H, W]\n",
    "\n",
    "            step4. Return processed image and label\n",
    "        \"\"\"\n",
    "\n",
    "        img_path = os.path.join(self.root, self.img_name[index]) # + '.jpeg'\n",
    "        img = Image.open(img_path)\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            transform=transforms.Compose([\n",
    "                transforms.CenterCrop(410), # crops the center region of the image with a square size of height\n",
    "                # transforms.Resize(300), # (h, w) 512x512 pixels\n",
    "                transforms.RandomHorizontalFlip(), #  randomly flips the image horizontally with a 50% chance\n",
    "                transforms.RandomRotation(degrees=15), # randomly rotates the image by a maximum of 15 degree\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "                # Add an additional transformation to rescale pixel values to [0, 1]\n",
    "                # transforms.Lambda(lambda x: (x + 1.0) / 2.0)\n",
    "            ])\n",
    "        elif self.mode == 'valid':\n",
    "            transform=transforms.Compose([\n",
    "                transforms.CenterCrop(410),\n",
    "                # transforms.Resize(300),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "                # Add an additional transformation to rescale pixel values to [0, 1]\n",
    "                # transforms.Lambda(lambda x: (x + 1.0) / 2.0)\n",
    "            ])\n",
    "        else:\n",
    "            transform = transforms.Compose([\n",
    "                transforms.CenterCrop(410),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            ])\n",
    "        \n",
    "        img = transform(img) \n",
    "        \n",
    "        # print(img[0].shape)\n",
    "        \n",
    "        if self.label is not None:\n",
    "            label = self.label[index]\n",
    "            return img, label\n",
    "        else:\n",
    "            return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d59f43c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1067"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(514)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = RetinopathyLoader(root = \".\", mode=\"train\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "valid_dataset = RetinopathyLoader(root = \".\", mode=\"valid\")\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = RetinopathyLoader(root = \".\", mode=\"test\")\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# train_cnt = len(train_loader.dataset)\n",
    "# print(train_cnt) # 7995\n",
    "\n",
    "# valid_cnt = len(valid_loader.dataset)\n",
    "# valid_cnt # 1599\n",
    "\n",
    "test_cnt = len(test_loader.dataset)\n",
    "test_cnt # 1067"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "808b6b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_device():\n",
    "    return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "device = get_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bf835d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, stride, downsample=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channel, out_channel, kernel_size=(3, 3), stride=stride, padding=(1, 1), bias=False),\n",
    "            nn.BatchNorm2d(out_channel, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channel, out_channel, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
    "            nn.BatchNorm2d(out_channel, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        )\n",
    "\n",
    "        # downsample: makes input channel = out channel\n",
    "        if downsample:\n",
    "            self.down_sample = nn.Sequential(\n",
    "                nn.Conv2d(in_channel, out_channel, kernel_size=(1,1), stride=(2,2), bias=False),\n",
    "                nn.BatchNorm2d(out_channel)\n",
    "            )\n",
    "        else:\n",
    "            self.down_sample = nn.Identity()\n",
    "        self.ReLU = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x) # in=64 out=64 \n",
    "        residual = self.down_sample(x)\n",
    "        out = self.ReLU(out + residual)\n",
    "        return out\n",
    "\n",
    "def add_basic_blocks(in_channel, out_channel, stride, downsample):\n",
    "    basic_blocks = []\n",
    "    basic_blocks.append(BasicBlock(in_channel, out_channel, stride, downsample))\n",
    "    basic_blocks.append(BasicBlock(out_channel, out_channel, (1, 1), False))\n",
    "    return basic_blocks\n",
    "\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False),\n",
    "            nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True,),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), dilation=1, ceil_mode=False),\n",
    "        )\n",
    "        \n",
    "        # Each layer has two basic blocks\n",
    "        self.layer1 = nn.Sequential(*add_basic_blocks(64, 64, (1, 1), False))\n",
    "        self.layer2 = nn.Sequential(*add_basic_blocks(64, 128, (2, 2), True))\n",
    "        self.layer3 = nn.Sequential(*add_basic_blocks(128, 256, (2, 2), True))\n",
    "        self.layer4 = nn.Sequential(*add_basic_blocks(256, 512, (2, 2), True))\n",
    "\n",
    "        # self.avg_pool = nn.AvgPool2d(kernal_size=7, stride=1, padding=0)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1)) # (output_H, output_W)\n",
    "        self.fc = nn.Linear(512, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.conv1(x) # in=3 out=64 \n",
    "        output = self.layer1(output)\n",
    "        output = self.layer2(output)\n",
    "        output = self.layer3(output)\n",
    "        output = self.layer4(output)              # print(output.shape) (num, 512, n, n)\n",
    "        output = self.avg_pool(output)            # print(output.shape) (num, 512, 1, 1)\n",
    "        # out = nn.flatten(out)\n",
    "        output = output.view(output.shape[0], -1) # print(output.shape) (num, 512)\n",
    "        output = self.fc(output)                  # print(output.shape) (num, 2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c49ff5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_loop(cur_epoch, num_epoch, remaining_epoch, train_loader, test_loader, model, loss_fn, optimizer, train_acc_list, valid_acc_list):\n",
    "    best_acc = 0\n",
    "    for e in range(remaining_epoch):\n",
    "        global predicted_list\n",
    "        predicted_list = []\n",
    "        train_acc = train(train_loader, model, loss_fn, optimizer, train_acc_list)\n",
    "        valid_acc = valid(valid_loader, model, loss_fn, valid_acc_list)\n",
    "        if (e % (1 if (num_epoch // 10) == 0 else (num_epoch // 10))) == 0:\n",
    "            print(f'Epoch: {cur_epoch+e:5}, train_acc:{train_acc:.2f}%')\n",
    "            print(f'Epoch: {cur_epoch+e:5}, valid_acc:{valid_acc:.2f}%')\n",
    "        if valid_acc > best_acc:\n",
    "            global predicted_label\n",
    "            predicted_label = predicted_list\n",
    "            best_acc = valid_acc\n",
    "            file_name = \"ResNet18_\"+str(optimizer.__class__.__name__)+\".pth\"\n",
    "            file_folder = \"models/ResNet18\"\n",
    "            file_path = os.path.join(file_folder, file_name)\n",
    "            torch.save(model.state_dict(), file_path)\n",
    "            \n",
    "            with open('models/ResNet18/train_state.pkl', 'wb') as f:\n",
    "                pickle.dump({\n",
    "                    'num_epoch': num_epoch,\n",
    "                    'cur_epoch': cur_epoch + e,\n",
    "                    'remaining_epoch': num_epoch - (cur_epoch + e),\n",
    "                    'loss_fn': loss_fn,\n",
    "                    \n",
    "                    'train_acc_list': train_acc_list,\n",
    "                    'valid_acc_list': valid_acc_list,\n",
    "                    'predicted_list': predicted_list,\n",
    "                    'predicted_label': predicted_label,\n",
    "                    'test_predicted_label': test_predicted_label,\n",
    "                    \n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict()\n",
    "                }, f)\n",
    "        if (e+1 == num_epoch or e % (1 if (num_epoch // 10) == 0 else (num_epoch // 10)))==0:\n",
    "            print(f'best acc: {best_acc:.2f}%')\n",
    "            \n",
    "def train(train_loader, model, loss_fn, optimizer, train_acc_list):\n",
    "    model.train() \n",
    "    match_train_cnt = 0\n",
    "    total_cnt = len(train_loader.dataset)\n",
    "    \n",
    "    for i, (train_data, train_label) in enumerate(train_loader):\n",
    "        train_data = train_data.to(device)\n",
    "        train_label = train_label.to(device).to(torch.long)\n",
    "\n",
    "        output = model(train_data)\n",
    "        loss = loss_fn(output, train_label)\n",
    "\n",
    "        pred = torch.max(output, dim=1) \n",
    "        match_train_cnt += (pred[1] == train_label).sum().item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    acc = match_train_cnt / total_cnt * 100\n",
    "    train_acc_list.append(acc)\n",
    "    return acc\n",
    "        \n",
    "def valid(valid_loader, model, loss_fn, valid_acc_list):\n",
    "    # Step 5: Testing loop\n",
    "    model.eval()\n",
    "    match_valid_cnt = 0\n",
    "    total_cnt = len(valid_loader.dataset)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (valid_data, valid_label) in enumerate(valid_loader):   \n",
    "            valid_data = valid_data.to(device)\n",
    "            valid_label = valid_label.to(device).to(torch.long)\n",
    "            output = model(valid_data)\n",
    "            pred = torch.max(output, dim=1)\n",
    "            match_valid_cnt += (pred[1] == valid_label).sum().item()\n",
    "            \n",
    "            # Convert the ground truth labels and predicted labels to NumPy arrays and \n",
    "            # Retrieve the indices corresponding to their maximum values\n",
    "            # true_label.extend(valid_label.cpu().data.numpy())\n",
    "            predicted_list.append(pred[1].cpu().data.numpy())\n",
    "\n",
    "    acc = match_valid_cnt / total_cnt * 100\n",
    "    valid_acc_list.append(acc)\n",
    "    return acc\n",
    "\n",
    "def test(test_loader, model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, test_data in enumerate(test_loader):\n",
    "            test_data = test_data.to(device)\n",
    "            output = model(test_data)\n",
    "            pred = torch.max(output, dim=1)\n",
    "            test_predicted_label.append(pred[1].cpu().data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ad02af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_result():\n",
    "    best_train_acc = 0\n",
    "    best_valid_acc = 0\n",
    "    for i in range(num_epoch):\n",
    "        best_train_acc = max(best_train_acc, train_acc_list[i])\n",
    "        best_valid_acc = max(best_valid_acc, valid_acc_list[i])\n",
    "    \n",
    "    plt.title('Accuracy Curve(ResNet18)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy(%)')\n",
    "\n",
    "    plt.plot(train_acc_list, label = 'ResNet18_train', color = 'b')\n",
    "    plt.plot(valid_acc_list, label = 'ResNet18_valid', color = 'r')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'Best train acc: {best_train_acc:.2f}%, Best valid acc: {best_valid_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61bf56a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(true_label, predicted_label, classes, normalize = False, title = None, cmap = plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(true_label, predicted_label)\n",
    "    \n",
    "    # Calculate class-wise accuracy if normalization is enabled\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98ca76dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_list = []\n",
    "valid_acc_list = []\n",
    "predicted_list = []\n",
    "predicted_label = []\n",
    "test_predicted_label = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ddf9ebe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:     0, train_acc:75.36%\n",
      "Epoch:     0, valid_acc:78.55%\n",
      "best acc: 78.55%\n",
      "Epoch:    20, train_acc:90.77%\n",
      "Epoch:    20, valid_acc:89.06%\n",
      "best acc: 90.62%\n",
      "Epoch:    40, train_acc:93.03%\n",
      "Epoch:    40, valid_acc:88.93%\n",
      "best acc: 90.99%\n",
      "Epoch:    60, train_acc:94.56%\n",
      "Epoch:    60, valid_acc:79.80%\n",
      "best acc: 92.81%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-4\u001b[39m, momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m epoch_loop(cur_epoch, num_epoch, remaining_epoch, train_loader, valid_loader, model, loss_fn, optimizer, train_acc_list, valid_acc_list)\n",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m, in \u001b[0;36mepoch_loop\u001b[1;34m(cur_epoch, num_epoch, remaining_epoch, train_loader, test_loader, model, loss_fn, optimizer, train_acc_list, valid_acc_list)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m predicted_list\n\u001b[0;32m      5\u001b[0m predicted_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 6\u001b[0m train_acc \u001b[38;5;241m=\u001b[39m train(train_loader, model, loss_fn, optimizer, train_acc_list)\n\u001b[0;32m      7\u001b[0m valid_acc \u001b[38;5;241m=\u001b[39m valid(valid_loader, model, loss_fn, valid_acc_list)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (e \u001b[38;5;241m%\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (num_epoch \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m (num_epoch \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10\u001b[39m))) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[1;32mIn[5], line 44\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_loader, model, loss_fn, optimizer, train_acc_list)\u001b[0m\n\u001b[0;32m     41\u001b[0m match_train_cnt \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     42\u001b[0m total_cnt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[1;32m---> 44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (train_data, train_label) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m     45\u001b[0m     train_data \u001b[38;5;241m=\u001b[39m train_data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     46\u001b[0m     train_label \u001b[38;5;241m=\u001b[39m train_label\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mlong)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[1], line 108\u001b[0m, in \u001b[0;36mRetinopathyLoader.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    102\u001b[0m     transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m    103\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mCenterCrop(\u001b[38;5;241m410\u001b[39m),\n\u001b[0;32m    104\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m    105\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m), std\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m)),\n\u001b[0;32m    106\u001b[0m     ])\n\u001b[1;32m--> 108\u001b[0m img \u001b[38;5;241m=\u001b[39m transform(img) \n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# print(img[0].shape)\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:393\u001b[0m, in \u001b[0;36mCenterCrop.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m    386\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be cropped.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Cropped image.\u001b[39;00m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcenter_crop(img, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\functional.py:607\u001b[0m, in \u001b[0;36mcenter_crop\u001b[1;34m(img, output_size)\u001b[0m\n\u001b[0;32m    605\u001b[0m crop_top \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mround\u001b[39m((image_height \u001b[38;5;241m-\u001b[39m crop_height) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2.0\u001b[39m))\n\u001b[0;32m    606\u001b[0m crop_left \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mround\u001b[39m((image_width \u001b[38;5;241m-\u001b[39m crop_width) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2.0\u001b[39m))\n\u001b[1;32m--> 607\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m crop(img, crop_top, crop_left, crop_height, crop_width)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\functional.py:564\u001b[0m, in \u001b[0;36mcrop\u001b[1;34m(img, top, left, height, width)\u001b[0m\n\u001b[0;32m    562\u001b[0m     _log_api_usage_once(crop)\n\u001b[0;32m    563\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m--> 564\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F_pil\u001b[38;5;241m.\u001b[39mcrop(img, top, left, height, width)\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mcrop(img, top, left, height, width)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torchvision\\transforms\\_functional_pil.py:235\u001b[0m, in \u001b[0;36mcrop\u001b[1;34m(img, top, left, height, width)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_pil_image(img):\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg should be PIL Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(img)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mcrop((left, top, left \u001b[38;5;241m+\u001b[39m width, top \u001b[38;5;241m+\u001b[39m height))\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\PIL\\Image.py:1233\u001b[0m, in \u001b[0;36mImage.crop\u001b[1;34m(self, box)\u001b[0m\n\u001b[0;32m   1230\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCoordinate \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlower\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is less than \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mupper\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1231\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m-> 1233\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m   1234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_crop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mim, box))\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\PIL\\ImageFile.py:249\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 249\u001b[0m         s \u001b[38;5;241m=\u001b[39m read(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecodermaxblock)\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIndexError\u001b[39;00m, struct\u001b[38;5;241m.\u001b[39merror) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    251\u001b[0m         \u001b[38;5;66;03m# truncated png/gif\u001b[39;00m\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m LOAD_TRUNCATED_IMAGES:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training step\n",
    "\n",
    "cur_epoch = 0\n",
    "num_epoch = 201\n",
    "remaining_epoch = num_epoch - cur_epoch\n",
    "\n",
    "model = ResNet18().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001, weight_decay=5e-4, momentum=0.9)\n",
    "epoch_loop(cur_epoch, num_epoch, remaining_epoch, train_loader, valid_loader, model, loss_fn, optimizer, train_acc_list, valid_acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f20694d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current epoch: 70, Remaining epoch: 131\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0001\u001b[39m, weight_decay\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5e-4\u001b[39m, momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m)\n\u001b[0;32m     30\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mload_state_dict(optimizer_state_dict)\n\u001b[1;32m---> 32\u001b[0m epoch_loop(cur_epoch, num_epoch, remaining_epoch, train_loader, valid_loader, model, loss_fn, optimizer, train_acc_list, valid_acc_list)\n",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m, in \u001b[0;36mepoch_loop\u001b[1;34m(cur_epoch, num_epoch, remaining_epoch, train_loader, test_loader, model, loss_fn, optimizer, train_acc_list, valid_acc_list)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m predicted_list\n\u001b[0;32m      5\u001b[0m predicted_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 6\u001b[0m train_acc \u001b[38;5;241m=\u001b[39m train(train_loader, model, loss_fn, optimizer, train_acc_list)\n\u001b[0;32m      7\u001b[0m valid_acc \u001b[38;5;241m=\u001b[39m valid(valid_loader, model, loss_fn, valid_acc_list)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (e \u001b[38;5;241m%\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (num_epoch \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m (num_epoch \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m10\u001b[39m))) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[1;32mIn[5], line 44\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_loader, model, loss_fn, optimizer, train_acc_list)\u001b[0m\n\u001b[0;32m     41\u001b[0m match_train_cnt \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     42\u001b[0m total_cnt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[1;32m---> 44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (train_data, train_label) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m     45\u001b[0m     train_data \u001b[38;5;241m=\u001b[39m train_data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     46\u001b[0m     train_label \u001b[38;5;241m=\u001b[39m train_label\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mlong)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn(data)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    205\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[38;5;241m=\u001b[39mdefault_collate_fn_map)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 119\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    160\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    161\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(batch, \u001b[38;5;241m0\u001b[39m, out\u001b[38;5;241m=\u001b[39mout)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Continue training from previous interrupted training\n",
    "# Or you can train more epochs by revising num_epoch\n",
    "\n",
    "with open('models/ResNet18/train_state.pkl', 'rb') as f:\n",
    "    train_state = pickle.load(f)\n",
    "    \n",
    "cur_epoch = train_state['cur_epoch']\n",
    "num_epoch = train_state['num_epoch']\n",
    "# num_epoch = 301\n",
    "# remaining_epoch = train_state['remaining_epoch']\n",
    "remaining_epoch = num_epoch - cur_epoch\n",
    "loss_fn = train_state['loss_fn']\n",
    "\n",
    "if remaining_epoch != 0:\n",
    "    print(f'Current epoch: {cur_epoch}, Remaining epoch: {remaining_epoch}')\n",
    "\n",
    "    train_acc_list = train_state['train_acc_list']\n",
    "    valid_acc_list = train_state['valid_acc_list']\n",
    "    predicted_list = train_state['predicted_list']\n",
    "    predicted_label = train_state['predicted_label']\n",
    "    test_predicted_label = train_state['test_predicted_label']\n",
    "\n",
    "    model_state_dict = train_state['model_state_dict']\n",
    "    optimizer_state_dict = train_state['optimizer_state_dict']\n",
    "\n",
    "    model = ResNet18().to(device)\n",
    "    model.load_state_dict(model_state_dict)\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.0001, weight_decay=5e-4, momentum=0.9)\n",
    "    optimizer.load_state_dict(optimizer_state_dict)\n",
    "\n",
    "    epoch_loop(cur_epoch, num_epoch, remaining_epoch, train_loader, valid_loader, model, loss_fn, optimizer, train_acc_list, valid_acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9374aab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo step: Run cell 1 to 7 and run this to show results on valid data\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('models/ResNet18/train_state.pkl', 'rb') as f:\n",
    "    train_state = pickle.load(f)\n",
    "\n",
    "num_epoch = train_state['num_epoch']\n",
    "loss_fn = train_state['loss_fn']\n",
    "\n",
    "model_state_dict = train_state['model_state_dict']\n",
    "optimizer_state_dict = train_state['optimizer_state_dict']\n",
    "\n",
    "model = ResNet18().to(device)\n",
    "model.load_state_dict(model_state_dict)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001, weight_decay=5e-4, momentum=0.9)\n",
    "optimizer.load_state_dict(optimizer_state_dict)\n",
    "\n",
    "train_acc_list = train_state['train_acc_list']\n",
    "valid_acc_list = train_state['valid_acc_list']\n",
    "predicted_list = train_state['predicted_list']\n",
    "predicted_label = train_state['predicted_label']\n",
    "test_predicted_label = train_state['test_predicted_label']\n",
    "\n",
    "show_result()\n",
    "\n",
    "_, true_label = getData('valid')\n",
    "predicted_label = np.concatenate(predicted_label).tolist()\n",
    "plot_confusion_matrix(true_label, predicted_label, ['0', '1'], title='Confusion matrix of ResNet18')\n",
    "plot_confusion_matrix(true_label, predicted_label, ['0', '1'], True, title='Confusion matrix of ResNet18')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6908d653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and run on test data then write to csv\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "model = ResNet18().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, weight_decay=5e-4, momentum=0.9)\n",
    "\n",
    "file_name = \"ResNet18_\"+str(optimizer.__class__.__name__)+\".pth\"\n",
    "file_folder = \"models/ResNet18\"\n",
    "file_path = os.path.join(file_folder, file_name)\n",
    "model.load_state_dict(torch.load(file_path))\n",
    "\n",
    "model.eval()\n",
    "test(test_loader, model)\n",
    "test_predicted_label = np.concatenate(test_predicted_label).tolist()\n",
    "\n",
    "# print(test_predicted_label)\n",
    "# print(len(test_predicted_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdc01fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def write_to_csv(image_paths, labels, output_file):\n",
    "    df = pd.DataFrame({'ID': image_paths, 'label': labels})\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "image_paths = getData('test')\n",
    "\n",
    "labels = test_predicted_label\n",
    "\n",
    "output_file = \"csv/resnet_18_test_SGD.csv\"\n",
    "\n",
    "write_to_csv(image_paths, labels, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

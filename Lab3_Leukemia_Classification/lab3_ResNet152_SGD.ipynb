{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce50a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import pickle\n",
    "\n",
    "def getData(mode):\n",
    "    if mode == 'train':\n",
    "        df = pd.read_csv('train.csv')\n",
    "        path = df['Path'].tolist()\n",
    "        label = df['label'].tolist()\n",
    "        return path, label\n",
    "    \n",
    "    elif mode == 'valid':\n",
    "        df = pd.read_csv('valid.csv')\n",
    "        path = df['Path'].tolist()\n",
    "        label = df['label'].tolist()\n",
    "        return path, label\n",
    "    \n",
    "    else:\n",
    "        df = pd.read_csv('resnet_18_test.csv')\n",
    "        path = df['Path'].tolist()\n",
    "        return path\n",
    "\n",
    "class RetinopathyLoader(data.Dataset):\n",
    "    def __init__(self, root, mode):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root (string): Root path of the dataset.\n",
    "            mode : Indicate procedure status(training or testing)\n",
    "\n",
    "            self.img_name (string list): String list that store all image names.\n",
    "            self.label (int or float list): Numerical list that store all ground truth label values.\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.mode = mode\n",
    "        \n",
    "        if mode == 'train' or mode == 'valid':\n",
    "            self.img_name, self.label = getData(mode)\n",
    "        else:  # mode == 'test'\n",
    "            self.img_name = getData(mode)\n",
    "            self.label = None  # No labels for test data\n",
    "            \n",
    "        # print(\"> Found %d images...\" % (len(self.img_name)))\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"'return the size of dataset\"\"\"\n",
    "        return len(self.img_name)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        \"\"\"\n",
    "           step1. Get the image path from 'self.img_name' and load it.\n",
    "                  hint : path = root + self.img_name[index] + '.jpeg'\n",
    "\n",
    "           step2. Get the ground truth label from self.label\n",
    "\n",
    "           step3. Transform the .jpeg rgb images during the training phase, such as resizing, random flipping, \n",
    "                  rotation, cropping, normalization etc. But at the beginning, I suggest you follow the hints. \n",
    "\n",
    "                  In the testing phase, if you have a normalization process during the training phase, you only need \n",
    "                  to normalize the data. \n",
    "\n",
    "                  hints : Convert the pixel value to [0, 1]\n",
    "                          Transpose the image shape from [H, W, C] to [C, H, W]\n",
    "\n",
    "            step4. Return processed image and label\n",
    "        \"\"\"\n",
    "\n",
    "        img_path = os.path.join(self.root, self.img_name[index]) # + '.jpeg'\n",
    "        img = Image.open(img_path)\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            transform=transforms.Compose([\n",
    "                transforms.CenterCrop(410), # crops the center region of the image with a square size of height\n",
    "                # transforms.Resize(300), # (h, w) 512x512 pixels\n",
    "                transforms.RandomHorizontalFlip(), #  randomly flips the image horizontally with a 50% chance\n",
    "                transforms.RandomRotation(degrees=15), # randomly rotates the image by a maximum of 15 degree\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "                # Add an additional transformation to rescale pixel values to [0, 1]\n",
    "                # transforms.Lambda(lambda x: (x + 1.0) / 2.0)\n",
    "            ])\n",
    "        elif self.mode == 'valid':\n",
    "            transform=transforms.Compose([\n",
    "                transforms.CenterCrop(410),\n",
    "                # transforms.Resize(300),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "                # Add an additional transformation to rescale pixel values to [0, 1]\n",
    "                # transforms.Lambda(lambda x: (x + 1.0) / 2.0)\n",
    "            ])\n",
    "        else:\n",
    "            transform = transforms.Compose([\n",
    "                transforms.CenterCrop(410),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "            ])\n",
    "        \n",
    "        img = transform(img) \n",
    "        \n",
    "        # print(img[0].shape)\n",
    "        \n",
    "        if self.label is not None:\n",
    "            label = self.label[index]\n",
    "            return img, label\n",
    "        else:\n",
    "            return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53447c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(514)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = RetinopathyLoader(root = \".\", mode=\"train\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "valid_dataset = RetinopathyLoader(root = \".\", mode=\"valid\")\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = RetinopathyLoader(root = \".\", mode=\"test\")\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# train_cnt = len(train_loader.dataset)\n",
    "# print(train_cnt) # 7995\n",
    "\n",
    "# valid_cnt = len(valid_loader.dataset)\n",
    "# valid_cnt # 1599\n",
    "\n",
    "test_cnt = len(test_loader.dataset)\n",
    "test_cnt # 1067"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf65871c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "device = get_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d94ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleneckBlock(nn.Module):\n",
    "    def __init__(self, prev_channel, in_channel, out_channel, stride=(1, 1), downsample=False):\n",
    "        super().__init__()\n",
    "                \n",
    "        # First bottleneck block needs to be downsampled\n",
    "        if downsample: \n",
    "            self.conv = nn.Sequential(\n",
    "                nn.Conv2d(prev_channel, out_channel, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
    "                nn.BatchNorm2d(out_channel, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                nn.Conv2d(out_channel, out_channel, kernel_size=(3, 3), stride=stride, padding=(1, 1), bias=False),\n",
    "                nn.BatchNorm2d(out_channel, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                nn.Conv2d(out_channel, in_channel, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
    "                nn.BatchNorm2d(in_channel, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "            )\n",
    "            self.down_sample = nn.Sequential(\n",
    "                nn.Conv2d(prev_channel, in_channel, kernel_size=(1,1), stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(in_channel)\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                nn.Conv2d(in_channel, out_channel, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
    "                nn.BatchNorm2d(out_channel, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                nn.Conv2d(out_channel, out_channel, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False),\n",
    "                nn.BatchNorm2d(out_channel, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "                nn.Conv2d(out_channel, in_channel, kernel_size=(1, 1), stride=(1, 1), bias=False),\n",
    "                nn.BatchNorm2d(in_channel, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "            )\n",
    "            self.down_sample = nn.Identity()\n",
    "            \n",
    "        self.ReLU = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        residual = self.down_sample(x)\n",
    "        out = self.ReLU(out + residual)\n",
    "        return out\n",
    "\n",
    "def add_bottleneck_blocks(prev_channel, in_channel, out_channel, stride, num_blocks):\n",
    "    bottleneck_blocks = []\n",
    "    bottleneck_blocks.append(BottleneckBlock(prev_channel, in_channel, out_channel, stride, True))\n",
    "    for _ in range(num_blocks-1):\n",
    "        bottleneck_blocks.append(BottleneckBlock(prev_channel, in_channel, out_channel))\n",
    "    return bottleneck_blocks\n",
    "\n",
    "class ResNet152(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False),\n",
    "            nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True,),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), dilation=1, ceil_mode=False),\n",
    "        )\n",
    "        \n",
    "        # Each layer has ($num_blocks) bottleneck blocks\n",
    "        self.layer1 = nn.Sequential(*add_bottleneck_blocks(64, 256, 64, (1, 1), num_blocks=3))\n",
    "        self.layer2 = nn.Sequential(*add_bottleneck_blocks(256, 512, 128, (2, 2), num_blocks=8)) \n",
    "        self.layer3 = nn.Sequential(*add_bottleneck_blocks(512, 1024, 256, (2, 2), num_blocks=36))\n",
    "        self.layer4 = nn.Sequential(*add_bottleneck_blocks(1024, 2048, 512, (2, 2), num_blocks=3))\n",
    "\n",
    "        # self.avg_pool = nn.AvgPool2d(kernal_size=7, stride=1, padding=0)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(output_size=(1, 1)) # (output_H, output_W)\n",
    "        self.fc = nn.Linear(2048, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.conv1(x)\n",
    "        output = self.layer1(output)\n",
    "        output = self.layer2(output)\n",
    "        output = self.layer3(output)\n",
    "        output = self.layer4(output)              # print(output.shape) (num, 512, n, n)\n",
    "        output = self.avg_pool(output)            # print(output.shape) (num, 512, 1, 1)\n",
    "        # out = nn.flatten(out)\n",
    "        output = output.view(output.shape[0], -1) # print(output.shape) (num, 512)\n",
    "        output = self.fc(output)                  # print(output.shape) (num, 2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5061404",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_loop(cur_epoch, num_epoch, remaining_epoch, train_loader, test_loader, model, loss_fn, optimizer, train_acc_list, valid_acc_list):\n",
    "    best_acc = 0\n",
    "    for e in range(remaining_epoch):\n",
    "        global predicted_list\n",
    "        predicted_list = []\n",
    "        train_acc = train(train_loader, model, loss_fn, optimizer, train_acc_list)\n",
    "        valid_acc = valid(valid_loader, model, loss_fn, valid_acc_list)\n",
    "        if (e % (1 if (num_epoch // 10) == 0 else (num_epoch // 10))) == 0:\n",
    "            print(f'Epoch: {cur_epoch+e:5}, train_acc:{train_acc:.2f}%')\n",
    "            print(f'Epoch: {cur_epoch+e:5}, valid_acc:{valid_acc:.2f}%')\n",
    "        if valid_acc > best_acc:\n",
    "            global predicted_label\n",
    "            predicted_label = predicted_list\n",
    "            best_acc = valid_acc\n",
    "            file_name = \"ResNet152_\"+str(optimizer.__class__.__name__)+\".pth\"\n",
    "            file_folder = \"models/ResNet152\"\n",
    "            file_path = os.path.join(file_folder, file_name)\n",
    "            torch.save(model.state_dict(), file_path)\n",
    "            \n",
    "            with open('models/ResNet152/train_state.pkl', 'wb') as f:\n",
    "                pickle.dump({\n",
    "                    'num_epoch': num_epoch,\n",
    "                    'cur_epoch': cur_epoch + e,\n",
    "                    'remaining_epoch': num_epoch - (cur_epoch + e),\n",
    "                    'loss_fn': loss_fn,\n",
    "                    \n",
    "                    'train_acc_list': train_acc_list,\n",
    "                    'valid_acc_list': valid_acc_list,\n",
    "                    'predicted_list': predicted_list,\n",
    "                    'predicted_label': predicted_label,\n",
    "                    'test_predicted_label': test_predicted_label,\n",
    "                    \n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict()\n",
    "                }, f)\n",
    "        if (e+1 == num_epoch or e % (1 if (num_epoch // 10) == 0 else (num_epoch // 10)))==0:\n",
    "            print(f'best acc: {best_acc:.2f}%')\n",
    "            \n",
    "def train(train_loader, model, loss_fn, optimizer, train_acc_list):\n",
    "    model.train() \n",
    "    match_train_cnt = 0\n",
    "    total_cnt = len(train_loader.dataset)\n",
    "    \n",
    "    for i, (train_data, train_label) in enumerate(train_loader):\n",
    "        train_data = train_data.to(device)\n",
    "        train_label = train_label.to(device).to(torch.long)\n",
    "\n",
    "        output = model(train_data)\n",
    "        loss = loss_fn(output, train_label)\n",
    "\n",
    "        pred = torch.max(output, dim=1) \n",
    "        match_train_cnt += (pred[1] == train_label).sum().item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    acc = match_train_cnt / total_cnt * 100\n",
    "    train_acc_list.append(acc)\n",
    "    return acc\n",
    "        \n",
    "def valid(valid_loader, model, loss_fn, valid_acc_list):\n",
    "    # Step 5: Testing loop\n",
    "    model.eval()\n",
    "    match_valid_cnt = 0\n",
    "    total_cnt = len(valid_loader.dataset)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (valid_data, valid_label) in enumerate(valid_loader):   \n",
    "            valid_data = valid_data.to(device)\n",
    "            valid_label = valid_label.to(device).to(torch.long)\n",
    "            output = model(valid_data)\n",
    "            pred = torch.max(output, dim=1)\n",
    "            match_valid_cnt += (pred[1] == valid_label).sum().item()\n",
    "            \n",
    "            # Convert the ground truth labels and predicted labels to NumPy arrays and \n",
    "            # Retrieve the indices corresponding to their maximum values\n",
    "            # true_label.extend(valid_label.cpu().data.numpy())\n",
    "            predicted_list.append(pred[1].cpu().data.numpy())\n",
    "\n",
    "    acc = match_valid_cnt / total_cnt * 100\n",
    "    valid_acc_list.append(acc)\n",
    "    return acc\n",
    "\n",
    "def test(test_loader, model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, test_data in enumerate(test_loader):\n",
    "            test_data = test_data.to(device)\n",
    "            output = model(test_data)\n",
    "            pred = torch.max(output, dim=1)\n",
    "            test_predicted_label.append(pred[1].cpu().data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f621f4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_result():\n",
    "    best_train_acc = 0\n",
    "    best_valid_acc = 0\n",
    "    for i in range(num_epoch):\n",
    "        best_train_acc = max(best_train_acc, train_acc_list[i])\n",
    "        best_valid_acc = max(best_valid_acc, valid_acc_list[i])\n",
    "    \n",
    "    plt.title('Accuracy Curve(ResNet152)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy(%)')\n",
    "\n",
    "    plt.plot(train_acc_list, label = 'ResNet152_train', color = 'b')\n",
    "    plt.plot(valid_acc_list, label = 'ResNet152_valid', color = 'r')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'Best train acc: {best_train_acc:.2f}%, Best valid acc: {best_valid_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e47da7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(true_label, predicted_label, classes, normalize = False, title = None, cmap = plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(true_label, predicted_label)\n",
    "    \n",
    "    # Calculate class-wise accuracy if normalization is enabled\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d4d09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_list = []\n",
    "valid_acc_list = []\n",
    "predicted_list = []\n",
    "predicted_label = []\n",
    "test_predicted_label = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd490c5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Training step\n",
    "\n",
    "cur_epoch = 0\n",
    "num_epoch = 201\n",
    "remaining_epoch = num_epoch - cur_epoch\n",
    "\n",
    "model = ResNet152().to(device)\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001, weight_decay=5e-4, momentum=0.9)\n",
    "epoch_loop(cur_epoch, num_epoch, remaining_epoch, train_loader, valid_loader, model, loss_fn, optimizer, train_acc_list, valid_acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d623d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue training from previous interrupted training\n",
    "# Or you can train more epochs by revising num_epoch\n",
    "\n",
    "with open('models/ResNet152/train_state.pkl', 'rb') as f:\n",
    "    train_state = pickle.load(f)\n",
    "    \n",
    "cur_epoch = train_state['cur_epoch']\n",
    "num_epoch = train_state['num_epoch']\n",
    "# num_epoch = 301\n",
    "# remaining_epoch = train_state['remaining_epoch']\n",
    "remaining_epoch = num_epoch - cur_epoch\n",
    "loss_fn = train_state['loss_fn']\n",
    "\n",
    "if remaining_epoch != 0:\n",
    "    print(f'Current epoch: {cur_epoch}, Remaining epoch: {remaining_epoch}')\n",
    "\n",
    "    train_acc_list = train_state['train_acc_list']\n",
    "    valid_acc_list = train_state['valid_acc_list']\n",
    "    predicted_list = train_state['predicted_list']\n",
    "    predicted_label = train_state['predicted_label']\n",
    "    test_predicted_label = train_state['test_predicted_label']\n",
    "\n",
    "    model_state_dict = train_state['model_state_dict']\n",
    "    optimizer_state_dict = train_state['optimizer_state_dict']\n",
    "\n",
    "    model = ResNet152().to(device)\n",
    "    model.load_state_dict(model_state_dict)\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.0001, weight_decay=5e-4, momentum=0.9)\n",
    "    optimizer.load_state_dict(optimizer_state_dict)\n",
    "\n",
    "    epoch_loop(cur_epoch, num_epoch, remaining_epoch, train_loader, valid_loader, model, loss_fn, optimizer, train_acc_list, valid_acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e986d775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo step: Run cell 1 to 7 and run this to show results on valid data\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('models/ResNet152/train_state.pkl', 'rb') as f:\n",
    "    train_state = pickle.load(f)\n",
    "\n",
    "num_epoch = train_state['num_epoch']\n",
    "loss_fn = train_state['loss_fn']\n",
    "\n",
    "model_state_dict = train_state['model_state_dict']\n",
    "optimizer_state_dict = train_state['optimizer_state_dict']\n",
    "\n",
    "model = ResNet152().to(device)\n",
    "model.load_state_dict(model_state_dict)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.0001, weight_decay=5e-4, momentum=0.9)\n",
    "optimizer.load_state_dict(optimizer_state_dict)\n",
    "\n",
    "train_acc_list = train_state['train_acc_list']\n",
    "valid_acc_list = train_state['valid_acc_list']\n",
    "predicted_list = train_state['predicted_list']\n",
    "predicted_label = train_state['predicted_label']\n",
    "test_predicted_label = train_state['test_predicted_label']\n",
    "\n",
    "show_result()\n",
    "\n",
    "_, true_label = getData('valid')\n",
    "predicted_label = np.concatenate(predicted_label).tolist()\n",
    "plot_confusion_matrix(true_label, predicted_label, ['0', '1'], title='Confusion matrix of ResNet152')\n",
    "plot_confusion_matrix(true_label, predicted_label, ['0', '1'], True, title='Confusion matrix of ResNet152')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c2b928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model and run on test data then write to csv\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "model = ResNet152().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, weight_decay=5e-4, momentum=0.9)\n",
    "\n",
    "file_name = \"ResNet152_\"+str(optimizer.__class__.__name__)+\".pth\"\n",
    "file_folder = \"models/ResNet152\"\n",
    "file_path = os.path.join(file_folder, file_name)\n",
    "model.load_state_dict(torch.load(file_path))\n",
    "\n",
    "model.eval()\n",
    "test(test_loader, model)\n",
    "test_predicted_label = np.concatenate(test_predicted_label).tolist()\n",
    "\n",
    "# print(test_predicted_label)\n",
    "# print(len(test_predicted_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2228e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def write_to_csv(image_paths, labels, output_file):\n",
    "    df = pd.DataFrame({'ID': image_paths, 'label': labels})\n",
    "    df.to_csv(output_file, index=False)\n",
    "    \n",
    "image_paths = getData('test')\n",
    "\n",
    "labels = test_predicted_label\n",
    "\n",
    "output_file = \"csv/resnet_152_test_SGD.csv\"\n",
    "\n",
    "write_to_csv(image_paths, labels, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

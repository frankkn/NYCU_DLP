{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82bf291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class kl_annealing():\n",
    "    def __init__(self, _type, cycle=1, ratio=1):\n",
    "        self.cur_iter = -1\n",
    "\n",
    "        self.type = _type\n",
    "        self.cycle = cycle\n",
    "        self.ratio = ratio\n",
    "\n",
    "        self.total_iter = 40000\n",
    "        \n",
    "        self.beta_list = np.ones(self.total_iter)\n",
    "\n",
    "        if(self.type == 'Cyclical'):\n",
    "            self.frange_cycle_linear(self.total_iter, n_cycle=self.cycle, ratio=self.ratio)\n",
    "        elif(self.type == 'Monotonic'):\n",
    "            self.frange_cycle_linear(self.total_iter, n_cycle=1, ratio=0.25)\n",
    "        elif(self.type == 'None'):\n",
    "            self.beta_list = np.zeros(self.total_iter)\n",
    "            \n",
    "    def update(self):\n",
    "        self.cur_iter += 1\n",
    "    \n",
    "    def get_beta(self):\n",
    "        self.update()\n",
    "        return self.beta_list[self.cur_iter]\n",
    "\n",
    "\n",
    "    def frange_cycle_linear(self, n_iter, start=0.0, stop=1.0,  n_cycle=1, ratio=1):\n",
    "        period = n_iter / n_cycle\n",
    "        step = (stop - start) / (period * ratio)\n",
    "\n",
    "        for c in range(n_cycle):\n",
    "            v, i = start, 0\n",
    "            while v <= stop and (int(i + c * period) < n_iter):\n",
    "                self.beta_list[int(i + c * period)] = v\n",
    "                v += step\n",
    "                i += 1\n",
    "                \n",
    "                \n",
    "def plot_kl_annealing(num_epoch, beta_list):\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig=plt.figure(figsize=(8, 4.0))\n",
    "    stride = max( int(num_epoch / 8), 1)\n",
    "    plt.plot(beta_list, marker= 's', color='k', markevery=stride, lw=2,  mec='k', mew=1 , markersize=10)\n",
    "    plt.grid(True)\n",
    "    plt.xlabel('Iteration', fontsize=16)\n",
    "    plt.ylabel(\"$\\\\beta$\", fontsize=16)\n",
    "    plt.show()\n",
    "\n",
    "KL = kl_annealing('Cyclical', 4, 0.5)\n",
    "plot_kl_annealing(num_epoch=KL.total_iter, beta_list=KL.beta_list)\n",
    "\n",
    "KL = kl_annealing('Monotonic')\n",
    "plot_kl_annealing(num_epoch=KL.total_iter, beta_list=KL.beta_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b2e16e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import torch\n",
    "from torch import stack\n",
    "from torch.utils.data import Dataset as torchData\n",
    "\n",
    "from torchvision.datasets.folder import default_loader as imgloader\n",
    "from torch import stack\n",
    "def get_key(fp):\n",
    "    filename = fp.split('/')[-1]\n",
    "    filename = filename.split('.')[0].replace('frame', '')\n",
    "#     print(f'filename = {filename}')\n",
    "    filename = filename.replace('train_img\\\\', '')  # 移除 \"train_img\" 字串\n",
    "    return int(filename)\n",
    "\n",
    "class Dataset_Dance(torchData):\n",
    "    def __init__(self, root, transform, mode='train', video_len=7, partial=1.0):\n",
    "        super().__init__()\n",
    "        assert mode in ['train', 'val'], \"There is no such mode !!!\"\n",
    "        if mode == 'train':\n",
    "            self.img_folder     = sorted(glob(os.path.join(root, 'LAB4_Dataset/train/train_img/*.png')), key=get_key)\n",
    "            self.prefix = 'train'\n",
    "        elif mode == 'val':\n",
    "            self.img_folder     = sorted(glob(os.path.join(root, 'LAB4_Dataset/val/val_img/*.png')), key=get_key)\n",
    "            self.prefix = 'val'\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.partial = partial\n",
    "        self.video_len = video_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(len(self.img_folder) * self.partial) // self.video_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.img_folder[index]\n",
    "        \n",
    "        imgs = []\n",
    "        labels = []\n",
    "        for i in range(self.video_len):\n",
    "            label_list = self.img_folder[(index*self.video_len)+i].split('/')\n",
    "            label_list[-2] = self.prefix + '_label'\n",
    "            \n",
    "            img_name    = self.img_folder[(index*self.video_len)+i]\n",
    "            label_name = '/'.join(label_list)\n",
    "\n",
    "            imgs.append(self.transform(imgloader(img_name)))\n",
    "            labels.append(self.transform(imgloader(label_name)))\n",
    "        return stack(imgs), stack(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e2bdde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_dataloader():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((32, 64)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    dataset = Dataset_Dance(root=\".\", transform=transform, mode='train', video_len=16, \\\n",
    "                                            partial=0.4)\n",
    "\n",
    "    train_loader = DataLoader(dataset,\n",
    "                              batch_size=2,\n",
    "                              num_workers=4,\n",
    "                              drop_last=True,\n",
    "                              shuffle=False)  \n",
    "    return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df33b4c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader = train_dataloader()\n",
    "len(train_loader) # 23410 / 16 = 1463\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92a71477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\User\\\\Desktop\\\\NYCU_DLP\\\\Lab4_Conditional_VAE_for_Video_Prediction'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9315d455",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/292 [00:07<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Only 2D, 3D, 4D, 5D padding with non-constant padding are supported for now",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\Desktop\\NYCU_DLP\\Lab4_Conditional_VAE_for_Video_Prediction\\Trainer.py:347\u001b[0m\n\u001b[0;32m    340\u001b[0m parser\u001b[38;5;241m.\u001b[39madd_argument(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--kl_anneal_ratio\u001b[39m\u001b[38;5;124m'\u001b[39m,    \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mfloat\u001b[39m, default\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,              help\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    345\u001b[0m args \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse_args()\n\u001b[1;32m--> 347\u001b[0m main(args)\n",
      "File \u001b[1;32m~\\Desktop\\NYCU_DLP\\Lab4_Conditional_VAE_for_Video_Prediction\\Trainer.py:294\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m    292\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 294\u001b[0m     model\u001b[38;5;241m.\u001b[39mtraining_stage()\n",
      "File \u001b[1;32m~\\Desktop\\NYCU_DLP\\Lab4_Conditional_VAE_for_Video_Prediction\\Trainer.py:148\u001b[0m, in \u001b[0;36mVAE_Model.training_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    146\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    147\u001b[0m label \u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 148\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_one_step(img, label, adapt_TeacherForcing)\n\u001b[0;32m    150\u001b[0m beta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkl_annealing\u001b[38;5;241m.\u001b[39mget_beta()\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m adapt_TeacherForcing:\n",
      "File \u001b[1;32m~\\Desktop\\NYCU_DLP\\Lab4_Conditional_VAE_for_Video_Prediction\\Trainer.py:177\u001b[0m, in \u001b[0;36mVAE_Model.training_one_step\u001b[1;34m(self, img, label, adapt_TeacherForcing)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtraining_one_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, img, label, adapt_TeacherForcing):\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m--> 177\u001b[0m     generated_img, mu, logvar \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(img, label)\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;66;03m# Reconstruction loss (MSE loss)\u001b[39;00m\n\u001b[0;32m    180\u001b[0m     recon_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmse_criterion(generated_img, img)\n",
      "File \u001b[1;32m~\\Desktop\\NYCU_DLP\\Lab4_Conditional_VAE_for_Video_Prediction\\Trainer.py:116\u001b[0m, in \u001b[0;36mVAE_Model.forward\u001b[1;34m(self, img, label)\u001b[0m\n\u001b[0;32m    113\u001b[0m img_flat \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, channels, height, width)\n\u001b[0;32m    115\u001b[0m img_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe_transformation(img_flat)\n\u001b[1;32m--> 116\u001b[0m label_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_transformation(label)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# Combine image and label features\u001b[39;00m\n\u001b[0;32m    119\u001b[0m combined_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([img_features, label_features], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Desktop\\NYCU_DLP\\Lab4_Conditional_VAE_for_Video_Prediction\\modules\\modules.py:75\u001b[0m, in \u001b[0;36mLabel_Encoder.forward\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, image):\n\u001b[1;32m---> 75\u001b[0m     padded_image \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(image, (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreflect\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mforward(image)\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Only 2D, 3D, 4D, 5D padding with non-constant padding are supported for now"
     ]
    }
   ],
   "source": [
    "%run Trainer.py --DR LAB4_Dataset --save_root checkpoint --fast_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df264a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
